{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23ea35f4-9a3f-49e6-8d3a-caf3d73b2f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.3.0\n",
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import torch # 2.7.0 han, bharat is 2.5.0\n",
    "import coremltools as ct # 8.3 on bharat , 7.2 Han\n",
    "# has to be Python 3.12 , bharat python 3.10.12 . can't be python 3.13\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from coremltools.converters.mil import input_types\n",
    "print(ct.__version__)\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d8b44de-f65f-463d-9435-784fa2112e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"nvidia/parakeet-tdt-0.6b-v2\"\n",
    "model_name = \"nvidia/parakeet-tdt-0.6b-v3\"\n",
    "audio, sr = librosa.load(\"first_10_seconds.wav\", sr=16000)\n",
    "\n",
    "audio_length = torch.tensor(audio.shape)\n",
    "audio_tensor = torch.from_numpy(audio).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c97994-45ad-4666-91ca-44d2a81ab5fe",
   "metadata": {},
   "source": [
    "### Load ASR module and transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab51548f-b05f-47f6-8c74-14b31a2a4e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "[NeMo I 2025-08-20 17:07:43 mixins:181] Tokenizer SentencePieceTokenizer initialized with 8192 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-08-20 17:07:46 modelPT:180] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    use_lhotse: true\n",
      "    skip_missing_manifest_entries: true\n",
      "    input_cfg: null\n",
      "    tarred_audio_filepaths: null\n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    shuffle: true\n",
      "    num_workers: 2\n",
      "    pin_memory: true\n",
      "    max_duration: 10.0\n",
      "    min_duration: 1.0\n",
      "    text_field: answer\n",
      "    batch_duration: null\n",
      "    max_tps: null\n",
      "    use_bucketing: true\n",
      "    bucket_duration_bins: null\n",
      "    bucket_batch_size: null\n",
      "    num_buckets: 30\n",
      "    bucket_buffer_size: 20000\n",
      "    shuffle_buffer_size: 10000\n",
      "    \n",
      "[NeMo W 2025-08-20 17:07:46 modelPT:187] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    use_lhotse: true\n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    max_duration: 40.0\n",
      "    min_duration: 0.1\n",
      "    num_workers: 2\n",
      "    pin_memory: true\n",
      "    text_field: answer\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-08-20 17:07:46 features:305] PADDING: 0\n",
      "[NeMo I 2025-08-20 17:07:49 rnnt_models:226] Using RNNT Loss : tdt\n",
      "    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n",
      "[NeMo I 2025-08-20 17:07:49 rnnt_models:226] Using RNNT Loss : tdt\n",
      "    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-08-20 17:07:49 tdt_loop_labels_computer:300] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: CUDA is not available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-08-20 17:07:49 rnnt_models:226] Using RNNT Loss : tdt\n",
      "    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-08-20 17:07:49 tdt_loop_labels_computer:300] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: CUDA is not available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-08-20 17:07:52 save_restore_connector:275] Model EncDecRNNTBPEModel was successfully restored from /Users/kikow/.cache/huggingface/hub/models--nvidia--parakeet-tdt-0.6b-v3/snapshots/bc3e42c344d9127e85c2d2f92be914f57d741b59/parakeet-tdt-0.6b-v3.nemo.\n"
     ]
    }
   ],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "print(nemo_asr.__version__)\n",
    "\n",
    "asr_model = nemo_asr.models.ASRModel.from_pretrained(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e900ac54-8582-49b5-9212-314b5bbacad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_target_': 'nemo.collections.asr.modules.RNNTDecoder', 'normalization_mode': None, 'random_state_sampling': False, 'blank_as_pad': True, 'prednet': {'pred_hidden': 640, 'pred_rnn_layers': 2, 't_max': None, 'dropout': 0.2}, 'vocab_size': 8192}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr_model.cfg[\"decoder\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd8d161d-b717-4291-a5a5-f2615e69ef47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcribing: 100%|██████████| 1/1 [00:00<00:00,  1.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Hypothesis(score=0.0, y_sequence=tensor([], dtype=torch.int64), text='', dec_out=None, dec_state=None, timestamp=[], alignments=None, frame_confidence=None, token_confidence=None, word_confidence=None, length=0, y=None, lm_state=None, lm_scores=None, ngram_lm_state=None, tokens=None, last_token=None, token_duration=None, last_frame=None)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = asr_model.transcribe(['first_10_seconds.wav'])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "251e2e6e-b214-42d5-97a5-bd1d2103803b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncDecRNNTBPEModel(\n",
       "  (preprocessor): AudioToMelSpectrogramPreprocessor(\n",
       "    (featurizer): FilterbankFeatures()\n",
       "  )\n",
       "  (encoder): ConformerEncoder(\n",
       "    (pre_encode): ConvSubsampling(\n",
       "      (out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
       "        (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
       "        (6): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pos_enc): RelPositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x ConformerLayer(\n",
       "        (norm_feed_forward1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward1): ConformerFeedForward(\n",
       "          (linear1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (activation): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        )\n",
       "        (norm_conv): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv): ConformerConvolution(\n",
       "          (pointwise_conv1): Conv1d(1024, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          (depthwise_conv): CausalConv1D(1024, 1024, kernel_size=(9,), stride=(1,), groups=1024, bias=False)\n",
       "          (batch_norm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): Swish()\n",
       "          (pointwise_conv2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        )\n",
       "        (norm_self_att): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attn): RelPositionMultiHeadAttention(\n",
       "          (linear_q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (linear_k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (linear_v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (linear_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_pos): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (norm_feed_forward2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward2): ConformerFeedForward(\n",
       "          (linear1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (activation): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm_out): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): RNNTDecoder(\n",
       "    (prediction): ModuleDict(\n",
       "      (embed): Embedding(8193, 640, padding_idx=8192)\n",
       "      (dec_rnn): LSTMDropout(\n",
       "        (lstm): LSTM(640, 640, num_layers=2, dropout=0.2)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (joint): RNNTJoint(\n",
       "    (pred): Linear(in_features=640, out_features=640, bias=True)\n",
       "    (enc): Linear(in_features=1024, out_features=640, bias=True)\n",
       "    (joint_net): Sequential(\n",
       "      (0): ReLU(inplace=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=640, out_features=8198, bias=True)\n",
       "    )\n",
       "    (_loss): RNNTLoss(\n",
       "      (_loss): TDTLossNumba()\n",
       "    )\n",
       "    (_wer): WER()\n",
       "  )\n",
       "  (loss): RNNTLoss(\n",
       "    (_loss): TDTLossNumba()\n",
       "  )\n",
       "  (spec_augmentation): SpectrogramAugmentation(\n",
       "    (spec_augment): SpecAugment()\n",
       "  )\n",
       "  (wer): WER()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09c93af-2073-4568-a7c4-076793229c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64bdb7ae-e242-455e-b02b-a6cc5ac6b433",
   "metadata": {},
   "source": [
    "### Audio to MelSpectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eca487a-6783-4487-8fb6-bdfbe30bd866",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioToMelSpectrogramPreprocessor(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sample_rate=16000\n",
    "        self.preemph = 0.97\n",
    "        self.guard = 0\n",
    "        self.mag_power = 2.0\n",
    "        self.log_zero_guard_value = 5.960464477539063e-08\n",
    "        self.n_fft = 512\n",
    "        self.hop_length = 160\n",
    "        self.win_length = 400\n",
    "        self.center = True\n",
    "        self.nfilt = 128\n",
    "        self.lowfreq = 0\n",
    "        self.highfreq = self.sample_rate / 2\n",
    "        self.mel_norm=\"slaney\"\n",
    "        self.pad_value = 0.0\n",
    "        self.CONSTANT = 1e-05\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"filterbanks\",\n",
    "            torch.tensor(\n",
    "                librosa.filters.mel(\n",
    "                    sr=self.sample_rate, n_fft=self.n_fft, n_mels=self.nfilt, fmin=self.lowfreq, fmax=self.highfreq, norm=self.mel_norm\n",
    "                ),\n",
    "                dtype=torch.float,\n",
    "            ).unsqueeze(0)\n",
    "        )\n",
    "        self.register_buffer(\"window\", torch.hann_window(self.win_length, periodic=False))\n",
    "\n",
    "    def stft(self, x):\n",
    "        return torch.stft(\n",
    "            x,\n",
    "            n_fft=self.n_fft,\n",
    "            hop_length=self.hop_length,\n",
    "            win_length=self.win_length,\n",
    "            center=True,\n",
    "            window=self.window,\n",
    "            return_complex=True,\n",
    "        )\n",
    "\n",
    "    def get_seq_len(self, seq_len):\n",
    "        # Assuming that center is True is stft_pad_amount = 0\n",
    "        pad_amount = self.n_fft // 2 * 2\n",
    "        seq_len = torch.floor_divide((seq_len + pad_amount - self.n_fft), self.hop_length) + 1\n",
    "        return seq_len.to(dtype=torch.long)\n",
    "\n",
    "    def normalize_batch(self, x, seq_len):\n",
    "        x_mean = None\n",
    "        x_std = None\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        max_time = x.shape[2]\n",
    "\n",
    "        time_steps = torch.arange(max_time, device=x.device).unsqueeze(0).expand(batch_size, max_time)\n",
    "        # time_steps = torch.arange(x.size(2), device=x.device).unsqueeze(0).repeat(x.size(0), 1)\n",
    "\n",
    "        valid_mask = time_steps < seq_len.unsqueeze(1)\n",
    "        x_mean_numerator = torch.where(valid_mask.unsqueeze(1), x, 0.0).sum(axis=2)\n",
    "        x_mean_denominator = valid_mask.sum(axis=1)\n",
    "        x_mean = x_mean_numerator / x_mean_denominator.unsqueeze(1)\n",
    "\n",
    "        # Subtract 1 in the denominator to correct for the bias.\n",
    "        x_std = torch.sqrt(\n",
    "            torch.sum(torch.where(valid_mask.unsqueeze(1), x - x_mean.unsqueeze(2), 0.0) ** 2, axis=2)\n",
    "            / (x_mean_denominator.unsqueeze(1) - 1.0)\n",
    "        )\n",
    "\n",
    "        # make sure x_std is not zero\n",
    "        x_std += self.CONSTANT\n",
    "        return (x - x_mean.unsqueeze(2)) / x_std.unsqueeze(2), x_mean, x_std\n",
    "\n",
    "    def audio_preprocessor_melspectrogram(self, input_signal, input_length):\n",
    "        seq_len = self.get_seq_len(input_length)\n",
    "\n",
    "        x = input_signal\n",
    "        x = torch.cat((x[:, 0].unsqueeze(1), x[:, 1:] - self.preemph * x[:, :-1]), dim=1)  # [1, 160000]\n",
    "        x = self.stft(x)    # [1, 257, 1001]\n",
    "        x = torch.view_as_real(x)\n",
    "        x = torch.sqrt(x.pow(2).sum(-1) + self.guard)\n",
    "        x = x.pow(self.mag_power)\n",
    "        x = torch.matmul(self.filterbanks, x)  # [1, 128, 1001]\n",
    "        x = torch.log(x + self.log_zero_guard_value)\n",
    "        x, _, _ = self.normalize_batch(x, seq_len)\n",
    "\n",
    "        max_len = x.size(-1)\n",
    "        mask = torch.arange(max_len, device=x.device)\n",
    "        mask = mask.repeat(x.size(0), 1) >= seq_len.unsqueeze(1)\n",
    "        x = x.masked_fill(mask.unsqueeze(1).type(torch.bool).to(device=x.device), self.pad_value)  # torch.Size([1, 128, 1001])\n",
    "        return (x, seq_len)\n",
    "\n",
    "    def forward(self, input_signals, input_length):\n",
    "        # This uses only the joint network (encoder+decoder → logits)\n",
    "        return self.audio_preprocessor_melspectrogram(input_signals, input_length)\n",
    "\n",
    "audio_melspectrogram_model = AudioToMelSpectrogramPreprocessor()\n",
    "\n",
    "example_inputs = (\n",
    "    audio_tensor,\n",
    "    audio_length\n",
    ")\n",
    "\n",
    "traced_melspectrogram = torch.jit.trace(audio_melspectrogram_model, example_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ad5c93a-7f62-4e31-a3ca-a7d668c7130b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tuple detected at graph output. This will be flattened in the converted model.\n",
      "fp64 dtype input audio_signal down casted to fp32.\n",
      "Converting PyTorch Frontend ==> MIL Ops:  99%|█████████▉| 212/214 [00:00<00:00, 3159.47 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 561.10 passes/s]\n",
      "Running MIL default pipeline:   0%|          | 0/89 [00:00<?, ? passes/s]Saving value type of float64 into a builtin type of fp32, might lose precision!\n",
      "Saving value type of float64 into a builtin type of fp32, might lose precision!\n",
      "Running MIL default pipeline: 100%|██████████| 89/89 [00:00<00:00, 340.45 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 822.09 passes/s]\n"
     ]
    }
   ],
   "source": [
    "import coremltools as ct\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Convert to CoreML model\n",
    "traced_melspectrogram.eval()\n",
    "mlmodel_melspectrogram = ct.convert(\n",
    "    traced_melspectrogram,\n",
    "    inputs=[\n",
    "        ct.TensorType(name=\"audio_signal\", shape=(1, ct.RangeDim(1, 160000)), dtype=np.float64),\n",
    "        ct.TensorType(name=\"audio_length\", shape=(1,), dtype=np.int32),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ct.TensorType(name=\"melspectrogram\"),\n",
    "        ct.TensorType(name=\"melspectrogram_length\"),\n",
    "    ],\n",
    "    convert_to=\"mlprogram\",  # more flexible backend\n",
    "    minimum_deployment_target=ct.target.iOS15  # iOS13 doesn't support mlprogram\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41d8fd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlmodel_melspectrogram.save(\"Melspectrogram_v2.mlpackage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92c29c2-7775-498c-a9a6-9e1bb72991cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e02ebe0-d0ce-4b37-bd3e-b03a16c65df1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'melspectrogram_length': array([1001], dtype=int32),\n",
       " 'melspectrogram': array([[[ 7.6207867 ,  2.4789796 ,  0.21312211, ...,  1.3899976 ,\n",
       "           4.3642735 ,  8.626841  ],\n",
       "         [ 8.278219  ,  2.3035228 ,  0.677722  , ...,  0.3853632 ,\n",
       "           4.041254  ,  9.412934  ],\n",
       "         [ 6.487728  ,  1.7332808 ,  0.87147015, ..., -1.091425  ,\n",
       "           2.5147805 ,  7.362334  ],\n",
       "         ...,\n",
       "         [-1.0946904 , -0.85021734,  1.1737236 , ...,  0.7718007 ,\n",
       "          -0.53026676, -2.3964825 ],\n",
       "         [-0.61513346,  0.6595316 ,  1.1478447 , ...,  0.54370254,\n",
       "          -2.6312695 , -1.3411158 ],\n",
       "         [-2.459402  , -0.8596429 ,  0.8162584 , ...,  1.056027  ,\n",
       "          -1.7531265 , -1.0641257 ]]], dtype=float32)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlmodel_spectrogram = ct.models.MLModel(\"Melspectrogram_v2.mlpackage\", compute_units=ct.ComputeUnit.CPU_AND_GPU)\n",
    "\n",
    "# Inputs must be a dictionary with keys matching model input names\n",
    "melspectrogram_dict = {\n",
    "    \"audio_signal\": audio_tensor.numpy().astype(np.float64),\n",
    "    \"audio_length\": audio_length.numpy().astype(np.int32),\n",
    "}\n",
    "\n",
    "output_spectrogram = mlmodel_spectrogram.predict(melspectrogram_dict)\n",
    "output_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80d64b3b-3eff-4373-9ef7-adefbcf21439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.7770,  2.6692,  0.3476,  ...,  1.4008,  4.4737,  8.7912],\n",
       "         [ 8.3773,  2.3284,  0.5868,  ...,  0.4289,  4.1233,  9.5138],\n",
       "         [ 6.4658,  1.5833,  0.7371,  ..., -0.7741,  2.5679,  7.3296],\n",
       "         ...,\n",
       "         [-1.0955, -0.8484,  1.1707,  ...,  0.7711, -0.5327, -2.4015],\n",
       "         [-0.6102,  0.6677,  1.1487,  ...,  0.5485, -2.6254, -1.3346],\n",
       "         [-2.4561, -0.8580,  0.8206,  ...,  1.0533, -1.7486, -1.0644]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc = traced_melspectrogram(audio_tensor, audio_length)\n",
    "abc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150c9795-3753-4402-bb39-f949f1cffa92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51857071-8fc3-4e68-bd1a-27f55ab75794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "974b8d66-8346-4512-b085-cc280d8fdc41",
   "metadata": {},
   "source": [
    "### Encoder Conversion to CoreML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "081ed1bc-7cc1-46aa-a91d-1ede53b8aca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, audio_signal, length):\n",
    "        # Get encoder output (shape: [B, H, T])\n",
    "        output = self.model(audio_signal=audio_signal, length=length)\n",
    "        if isinstance(output, tuple):\n",
    "            encoder_output, encoder_length = output\n",
    "        else:\n",
    "            encoder_output = output\n",
    "            encoder_length = length  # Use input length if not provided\n",
    "\n",
    "        # Transpose to [B, T, H] for compatibility with decoder\n",
    "        # CoreML/Swift expects this format\n",
    "        encoder_output = encoder_output.transpose(1, 2).contiguous()\n",
    "        return encoder_output, encoder_length\n",
    "\n",
    "asr_model.encoder.eval()\n",
    "model_encoder = EncoderWrapper(asr_model.encoder)\n",
    "\n",
    "example_inputs = (\n",
    "    torch.from_numpy(output_spectrogram[\"melspectrogram\"]),\n",
    "    torch.from_numpy(output_spectrogram[\"melspectrogram_length\"])\n",
    ")\n",
    "\n",
    "traced_encoder = torch.jit.trace(model_encoder, example_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6958e74e-015b-4f9c-879b-d6d524dd9938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting PyTorch Frontend ==> MIL Ops:   0%|          | 0/2851 [00:00<?, ? ops/s]Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 2849/2851 [00:01<00:00, 2832.69 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 25.60 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 89/89 [00:17<00:00,  5.06 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 27.92 passes/s]\n",
      "Running compression pass linear_quantize_weights: 100%|██████████| 614/614 [00:12<00:00, 48.09 ops/s]\n",
      "Running MIL frontend_milinternal pipeline: 0 passes [00:00, ? passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 87/87 [00:05<00:00, 16.85 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 24.78 passes/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved INT8 encoder to ParakeetEncoder_v2.mlpackage\n",
      "  Supports dynamic input: 0.1-10 seconds\n",
      "\n",
      "Verification:\n",
      "Output shape: (1, 126, 1024)\n",
      "Expected: [1, 126, 1024] (batch, time, hidden)\n",
      "✅ Transpose is working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Convert to CoreML with dynamic input\n",
    "traced_encoder.eval()\n",
    "mlmodel_encoder = ct.convert(\n",
    "  traced_encoder,\n",
    "    inputs=[\n",
    "        ct.TensorType(name=\"audio_signal\", shape=(1, 128, 1001), dtype=np.float32),\n",
    "        ct.TensorType(name=\"length\", shape=(1,), dtype=int)\n",
    "    ],\n",
    "    outputs=[\n",
    "        ct.TensorType(name=\"encoder_output\"),\n",
    "        ct.TensorType(name=\"encoder_output_length\"),\n",
    "    ],\n",
    "  compute_precision=ct.precision.FLOAT16,\n",
    "  minimum_deployment_target=ct.target.iOS15\n",
    ")\n",
    "\n",
    "# Apply INT8 quantization\n",
    "op_config = OpLinearQuantizerConfig(\n",
    "  mode=\"linear_symmetric\",\n",
    "  granularity=\"per_channel\",\n",
    "  weight_threshold=512,\n",
    ")\n",
    "config = OptimizationConfig(global_config=op_config)\n",
    "\n",
    "mlmodel_encoder_int8 = linear_quantize_weights(mlmodel_encoder, config=config)\n",
    "\n",
    "# Save the INT8 model\n",
    "mlmodel_encoder_int8.save(\"ParakeetEncoder_v2.mlpackage\")\n",
    "print(\"✓ Saved INT8 encoder to ParakeetEncoder_v2.mlpackage\")\n",
    "print(\"  Supports dynamic input: 0.1-10 seconds\")\n",
    "\n",
    "# Test the output shape to verify transpose is working\n",
    "test_mel = np.random.randn(1, 128, 1001).astype(np.float32)\n",
    "test_length = np.array([1001], dtype=np.int32)\n",
    "\n",
    "output = mlmodel_encoder_int8.predict({\n",
    "  'audio_signal': test_mel,\n",
    "  'length': test_length\n",
    "})\n",
    "\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"Output shape: {output['encoder_output'].shape}\")\n",
    "print(f\"Expected: [1, 126, 1024] (batch, time, hidden)\")\n",
    "if output['encoder_output'].shape == (1, 126, 1024):\n",
    "  print(\"✅ Transpose is working correctly!\")\n",
    "else:\n",
    "  print(\"❌ Transpose may not be working - check the shape\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee42794-0055-4cec-a051-d3e2a01f3b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlmodel_encoder = ct.models.MLModel(\"ParakeetEncoder.mlpackage\")\n",
    "\n",
    "# Inputs must be a dictionary with keys matching model input names\n",
    "input_dict = {\n",
    "    \"melspectrogram\": output_spectrogram[\"melspectrogram\"],\n",
    "    \"melspectrogram_length\": output_spectrogram[\"melspectrogram_length\"]\n",
    "}\n",
    "\n",
    "outputs_encoder = mlmodel_encoder.predict(input_dict)\n",
    "outputs_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b333128-e51d-41ca-8538-e636d647ad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_encoder(torch.from_numpy(output_spectrogram[\"melspectrogram\"]), torch.from_numpy(output_spectrogram[\"melspectrogram_length\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c047e0bc-6bc0-455b-a6c1-e04518748f7a",
   "metadata": {},
   "source": [
    "### Decoder conversion to CoreML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35acdaa3-65ae-43c1-80b5-755eaf842b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWrapper(nn.Module):\n",
    "    def __init__(self, decoder):\n",
    "        super().__init__()\n",
    "        self.embed = decoder.prediction[\"embed\"]\n",
    "        self.rnn = decoder.prediction[\"dec_rnn\"].lstm  # <== Access underlying LSTM directly\n",
    "\n",
    "    def forward(self, targets, target_lengths, h_in, c_in):\n",
    "        embedded = self.embed(targets)  # (B, T, D)\n",
    "        output, (h_out, c_out) = self.rnn(embedded, (h_in, c_in))  # Run LSTM manually\n",
    "        return output, h_out, c_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d51bd65-c810-4b1c-b72a-aa71298e55c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_decoder = DecoderWrapper(asr_model.decoder)\n",
    "\n",
    "vocab_size = asr_model.decoder.prediction['embed'].num_embeddings\n",
    "hidden_size = asr_model.cfg[\"decoder\"][\"prednet\"][\"pred_hidden\"]     # 640\n",
    "num_layers = asr_model.cfg[\"decoder\"][\"prednet\"][\"pred_rnn_layers\"]  # 2\n",
    "\n",
    "batch = 1\n",
    "num_seq = 1\n",
    "\n",
    "example_inputs = (\n",
    "    torch.randint(0, vocab_size, (batch, num_seq), dtype=torch.int32),   # targets\n",
    "    torch.tensor([20], dtype=torch.int32),                    # target_lengths\n",
    "    torch.zeros(num_layers, num_seq, hidden_size, dtype=torch.float32),                   # h_in\n",
    "    torch.zeros(num_layers, num_seq, hidden_size, dtype=torch.float32),                   # c_in\n",
    ")\n",
    "\n",
    "wrapped_decoder.eval()\n",
    "traced_decoder = torch.jit.trace(wrapped_decoder, example_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bc7170-b611-40f9-b7a7-003c28043f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "traced_decoder.eval()\n",
    "mlmodel_decoder = ct.convert(\n",
    "    traced_decoder,\n",
    "    inputs=[\n",
    "        input_types.TensorType(\n",
    "            name=\"targets\",\n",
    "            shape=(1, input_types.RangeDim(1, 1000)),  # dynamic length\n",
    "            dtype=int\n",
    "        ),\n",
    "        input_types.TensorType(name=\"target_lengths\", shape=(1,), dtype=int),\n",
    "        input_types.TensorType(name=\"h_in\", shape=(num_layers, num_seq, hidden_size), dtype=np.float32),\n",
    "        input_types.TensorType(name=\"c_in\", shape=(num_layers, num_seq, hidden_size), dtype=np.float32)\n",
    "    ],\n",
    "    outputs=[\n",
    "        input_types.TensorType(name=\"decoder_output\"),\n",
    "        input_types.TensorType(name=\"h_out\"),\n",
    "        input_types.TensorType(name=\"c_out\"),\n",
    "    ],\n",
    "    convert_to=\"mlprogram\",\n",
    "    minimum_deployment_target=ct.target.iOS15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a15774e-5e19-4585-aa46-de1dc4ddb74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlmodel_decoder.save(\"ParakeetDecoder.mlpackage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d33553-34f2-41ba-9426-7b6d1101db4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlmodel_decoder = ct.models.MLModel(\"ParakeetDecoder.mlpackage\")\n",
    "\n",
    "# Inputs must be a dictionary with keys matching model input names\n",
    "input_dict = {\n",
    "    \"targets\": example_inputs[0],\n",
    "    \"target_lengths\": example_inputs[1],\n",
    "    \"h_in\": example_inputs[2],\n",
    "    \"c_in\": example_inputs[3]\n",
    "}\n",
    "\n",
    "output_decoder = mlmodel_decoder.predict(input_dict)\n",
    "for key in output_decoder.keys():\n",
    "    print (key, output_decoder[key].shape, output_decoder[key].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feacd3e0-86c8-4639-ace0-dcf197746b60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb28d9b-d9f8-4fd5-840f-32f6e779b26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "SIMPLE COPY-PASTE SCRIPT TO CREATE 30-SECOND ENCODER\n",
    "Just run: python3 create_30s_encoder.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import coremltools as ct\n",
    "from pathlib import Path\n",
    "import nemo.collections.asr as nemo_asr\n",
    "\n",
    "print(\"Creating 30-second encoder...\")\n",
    "\n",
    "# Load the model\n",
    "nemo_path = Path.home() / \".cache/huggingface/hub/models--nvidia--parakeet-tdt-0.6b-v3/snapshots/bb0964b98adc4b962566f3df9f5f1844011871a3/parakeet-tdt-0.6b-v3.nemo\"\n",
    "asr_model = nemo_asr.models.EncDecRNNTBPEModel.restore_from(str(nemo_path))\n",
    "asr_model.eval()\n",
    "\n",
    "# Create 30-second audio sample\n",
    "sample_audio_30s = torch.randn(1, 480000).float()  # 30 seconds at 16kHz\n",
    "sample_length_30s = torch.tensor([480000], dtype=torch.long)\n",
    "\n",
    "# Get 30-second mel-spectrogram\n",
    "with torch.no_grad():\n",
    "    mel_30s, mel_length_30s = asr_model.preprocessor(\n",
    "        input_signal=sample_audio_30s,\n",
    "        length=sample_length_30s\n",
    "    )\n",
    "\n",
    "print(f\"Mel-spectrogram shape: {mel_30s.shape}\")  # Should be [1, 128, 3001]\n",
    "\n",
    "# Create encoder wrapper\n",
    "class EncoderWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, audio_signal, length):\n",
    "        return self.model(audio_signal=audio_signal, length=length)\n",
    "\n",
    "# Wrap and trace encoder\n",
    "encoder_wrapper = EncoderWrapper(asr_model.encoder)\n",
    "encoder_wrapper.eval()\n",
    "\n",
    "traced_encoder = torch.jit.trace(encoder_wrapper, (mel_30s, mel_length_30s))\n",
    "traced_encoder.eval()\n",
    "\n",
    "# Convert to CoreML with 30-second shape\n",
    "print(\"Converting to CoreML...\")\n",
    "mlmodel_encoder = ct.convert(\n",
    "    traced_encoder,\n",
    "    inputs=[\n",
    "        ct.TensorType(name=\"melspectrogram\", shape=(1, 128, 3001), dtype=np.float32),  # 30 SECONDS!\n",
    "        ct.TensorType(name=\"melspectrogram_length\", shape=(1,), dtype=np.int32)\n",
    "    ],\n",
    "    outputs=[\n",
    "        ct.TensorType(name=\"encoder_output\"),\n",
    "        ct.TensorType(name=\"encoder_output_length\"),\n",
    "    ],\n",
    "    compute_units=ct.ComputeUnit.CPU_AND_NE,\n",
    "    minimum_deployment_target=ct.target.iOS15\n",
    ")\n",
    "\n",
    "# Save to FluidAudio models directory\n",
    "output_dir = Path.home() / \"Library/Application Support/FluidAudio/Models/parakeet-asr-coreml\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Backup existing encoder\n",
    "existing = output_dir / \"ParakeetEncoder_transposed_int8.mlpackage\"\n",
    "if existing.exists():\n",
    "    import shutil\n",
    "    backup = output_dir / \"ParakeetEncoder_10s_backup.mlpackage\"\n",
    "    if not backup.exists():\n",
    "        shutil.copytree(existing, backup)\n",
    "        print(f\"Backed up existing encoder\")\n",
    "\n",
    "# Save new encoder\n",
    "output_path = output_dir / \"ParakeetEncoder_30s.mlpackage\"\n",
    "mlmodel_encoder.save(str(output_path))\n",
    "print(f\"✅ Saved 30-second encoder to: {output_path}\")\n",
    "\n",
    "# Test it\n",
    "print(\"\\nTesting encoder...\")\n",
    "model = ct.models.MLModel(str(output_path))\n",
    "\n",
    "for seconds in [10, 20, 30]:\n",
    "    frames = seconds * 100 + 1\n",
    "    test_mel = np.zeros((1, 128, 3001), dtype=np.float32)\n",
    "    test_mel[:, :, :frames] = np.random.randn(1, 128, frames).astype(np.float32) * 0.1\n",
    "    test_length = np.array([frames], dtype=np.int32)\n",
    "\n",
    "    output = model.predict({\n",
    "        \"melspectrogram\": test_mel,\n",
    "        \"melspectrogram_length\": test_length\n",
    "    })\n",
    "\n",
    "    print(f\"  {seconds}s input → encoder length: {output['encoder_output_length'][0]}\")\n",
    "\n",
    "print(\"\\n✅ DONE! Your encoder now accepts 30-second inputs!\")\n",
    "print(\"\\nNext: swift build -c release\")\n",
    "print(\"Test: ./.build/release/fluidaudio asr-benchmark --max-files 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfee26e2-fc8a-4add-bb93-9a94e24f3536",
   "metadata": {},
   "source": [
    "### Joint Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9ce4d0-6934-4bc3-91ac-c579dc569ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointInferenceWrapper(torch.nn.Module):\n",
    "    def __init__(self, joint):\n",
    "        super().__init__()\n",
    "        self.joint = joint\n",
    "\n",
    "    def forward(self, encoder_outputs, decoder_outputs):\n",
    "        # This uses only the joint network (encoder+decoder → logits)\n",
    "        return self.joint.joint(encoder_outputs, decoder_outputs)\n",
    "\n",
    "wrapper_joint = JointInferenceWrapper(asr_model.joint)\n",
    "\n",
    "example_inputs = (\n",
    "    torch.randn(1, 10, 1024),  # encoder_outputs (B, T, D_enc)\n",
    "    torch.randn(1, 5, 640),    # decoder_outputs (B, U, D_pred)\n",
    ")\n",
    "\n",
    "wrapper_joint.eval()\n",
    "traced_joint = torch.jit.trace(wrapper_joint, example_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07a928a-ec6b-4406-a546-575f660b356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to CoreML model\n",
    "traced_joint.eval()\n",
    "mlmodel_joint = ct.convert(\n",
    "    traced_joint,\n",
    "    inputs=[\n",
    "        ct.TensorType(name=\"encoder_outputs\", shape=(ct.RangeDim(1, 100), ct.RangeDim(1, 1025), ct.RangeDim(1, 1024)), dtype=np.float32),\n",
    "        ct.TensorType(name=\"decoder_outputs\", shape=(ct.RangeDim(1, 100), ct.RangeDim(1, 1025), ct.RangeDim(1, 640)), dtype=np.float32),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ct.TensorType(name=\"logits\"),\n",
    "    ],\n",
    "    convert_to=\"mlprogram\",  # more flexible backend\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e188d91-9067-4d20-910d-c7ab271abe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlmodel_joint.save(\"RNNTJoint.mlpackage\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
